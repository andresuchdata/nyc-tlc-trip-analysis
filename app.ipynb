{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 2 Project - NYC TLC Trip Record Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "raw_df = pd.read_csv('NYC_TLC_Trip_Record.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('app.ipynb', 'r') as f:\n",
    "    notebook = json.load(f)\n",
    "print(f\"Number of cells: {len(notebook['cells'])}\")\n",
    "print(f\"Total size of outputs: {sum(len(str(cell.get('outputs', ''))) for cell in notebook['cells'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "## Show raw data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_df.describe())\n",
    "raw_df.info()\n",
    "\n",
    "df = raw_df.copy()\n",
    "\n",
    "print(\"\\nNumber of null values summary:\")\n",
    "display(raw_df.isnull().sum())\n",
    "\n",
    "print(\"\\nNumber of duplicated rows:\")\n",
    "display(df.duplicated().sum())\n",
    "\n",
    "print(\"\\nZone lookup table:\")\n",
    "# add zone name for pick up and drop-off columns\n",
    "df_zone = pd.read_csv('taxi_zone_lookup.csv')\n",
    "display(df_zone)\n",
    "\n",
    "\n",
    "# Join zone names for both pickup and dropoff locations\n",
    "df = df.merge(\n",
    "    df_zone[['LocationID', 'Zone']],\n",
    "    left_on='PULocationID',\n",
    "    right_on='LocationID',\n",
    "    how='left'\n",
    ").rename(columns={'Zone': 'PUZone'}).drop('LocationID', axis=1)\n",
    "\n",
    "df = df.merge(\n",
    "    df_zone[['LocationID', 'Zone']],\n",
    "    left_on='DOLocationID',\n",
    "    right_on='LocationID',\n",
    "    how='left'\n",
    ").rename(columns={'Zone': 'DOZone'}).drop('LocationID', axis=1)\n",
    "\n",
    "\n",
    "print(\"\\ndf with zone names\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several columns have missing data (63887 rows out of 68211 rows)\n",
    "\n",
    "Columns: \n",
    "- `RatecodeID`\n",
    "- `trip_type`\n",
    "- `store_and_fwd_flag`\n",
    "- `passenger_count`\n",
    "- `payment_type`\n",
    "- `congestion_surcharge`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process missing data for `RatecodeID` and `trip_type`\n",
    "\n",
    "### `RatecodeID`\n",
    "- 99 is an invalid data, so we can replace it with NaN first\n",
    "- Then, we can impute the missing data NaN `RatecodeID` with mode, i.e. 1 (most frequent value) because `RatecodeID` is categorical and the distribution for RatecodeID 1 is 91%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK frequency of RatecodeID values\n",
    "print(\"Check frequency of RatecodeID:\")\n",
    "display(raw_df['RatecodeID'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "print(\"Check distribution of RatecodeID:\")\n",
    "display(raw_df['RatecodeID'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "# show rows where RatecodeID is 99 (invalid data)\n",
    "display(raw_df[raw_df['RatecodeID'] == 99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's replace 99 with NaN\n",
    "df['RatecodeID'] = raw_df['RatecodeID'].replace(99, np.nan)\n",
    "\n",
    "# Now let's check the distribution again\n",
    "print(\"RatecodeID Distribution (after removing 99):\")\n",
    "display(df['RatecodeID'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "\n",
    "mode_ratecode = df['RatecodeID'].mode()[0]\n",
    "df['RatecodeID_imputed'] = df['RatecodeID'].fillna(mode_ratecode)\n",
    "\n",
    "print(\"\\nMode-imputed RatecodeID distribution - no more 99 or NA:\")\n",
    "display(df['RatecodeID_imputed'].value_counts(normalize=True))\n",
    "\n",
    "# Compare the imputation methods\n",
    "print(\"\\nComparison of imputation methods:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': raw_df['RatecodeID'].value_counts(normalize=True, dropna=False),\n",
    "    'Mode Imputed': df['RatecodeID_imputed'].value_counts(normalize=True),\n",
    "})\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `trip_type`\n",
    "- We observe that rows with `RatecodeID` is 99 (invalid data) also have `trip_type` as null\n",
    "- We can impute the missing data `trip_type` with mode, i.e. 1 (most frequent value) because `trip_type` is categorical and the distribution for trip_type 1 is already 91.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check trip_type for null values\n",
    "print(\"Trip Type Distribution:\")\n",
    "display(raw_df['trip_type'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "null_trip_type = raw_df[raw_df['trip_type'].isnull() & ~(raw_df['store_and_fwd_flag'].isnull())]\n",
    "\n",
    "print(\"Trip Type for null values joined with RatecodeID = 99 => it turns out to be the same set of rows\")\n",
    "display(raw_df[raw_df['RatecodeID'] == 99].index)\n",
    "display(null_trip_type.index)\n",
    "\n",
    "# we can impute trip_type with mode (most frequent value), which is 1 in this case\n",
    "df['trip_type_imputed'] = df['trip_type'].fillna(1)\n",
    "\n",
    "# after imputation, check the distribution\n",
    "print(\"Trip Type Frequency after mode imputation:\")\n",
    "display(df['trip_type_imputed'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "# Compare the original and imputed distributions\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': raw_df['trip_type'].value_counts(normalize=True, dropna=False),\n",
    "    'Imputed': df['trip_type_imputed'].value_counts(normalize=True),\n",
    "})\n",
    "\n",
    "print(\"Comparison of original and imputed trip_type distributions:\")\n",
    "display(comparison)\n",
    "\n",
    "# Optionally, you can visualize the comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "comparison['Original'].plot(kind='bar', ax=ax1, title='Original Distribution')\n",
    "comparison['Imputed'].plot(kind='bar', ax=ax2, title='Imputed Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning for `store_and_fwd_flag`\n",
    "- We can impute the missing data `store_and_fwd_flag` with mode, i.e. N (most frequent value) because `store_and_fwd_flag` is categorical and the distribution for store_and_fwd_flag N is 93.2%\n",
    "- This column distribution is mostly filled with N, so mode imputation is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Store and Fwd Flag Frequency:\")\n",
    "display(raw_df['store_and_fwd_flag'].value_counts(dropna=False))\n",
    "\n",
    "print(\"Store and Fwd Flag Distribution:\")\n",
    "display(raw_df['store_and_fwd_flag'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "mode_store_and_fwd_flag = df['store_and_fwd_flag'].mode()[0]\n",
    "df['store_and_fwd_flag_imputed'] = df['store_and_fwd_flag'].fillna(mode_store_and_fwd_flag)\n",
    "\n",
    "print(\"\\nstore_and_fwd_flag distribution:\")\n",
    "display(df['store_and_fwd_flag_imputed'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning for `payment_type`\n",
    "- There are 4324 null values in `payment_type`\n",
    "- Since `payment_type` distribution is only 59%, so using mode imputation is not a good idea\n",
    "- We better use probabilistic imputation to match original distribution as close as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Payment Type Frequency:\")\n",
    "display(raw_df['payment_type'].value_counts(dropna=False))\n",
    "\n",
    "print(\"Payment Type Distribution:\")\n",
    "display(raw_df['payment_type'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "# add probabilistic imputation\n",
    "# Calculate probabilities excluding NaN\n",
    "payment_type_probs = raw_df['payment_type'].value_counts(normalize=True).drop(index=np.nan, errors='ignore')\n",
    "# Normalize probabilities to sum to 1\n",
    "payment_type_probs = payment_type_probs / payment_type_probs.sum()\n",
    "nan_mask = df['payment_type'].isna()\n",
    "random_choices = np.random.choice(payment_type_probs.index, p=payment_type_probs.values, size=nan_mask.sum())\n",
    "\n",
    "# Create the new column, starting with the original values\n",
    "df['payment_type_imputed'] = df['payment_type']\n",
    "\n",
    "# Fill NaN values with the random choices\n",
    "df.loc[nan_mask, 'payment_type_imputed'] = random_choices\n",
    "\n",
    "# Verify the result\n",
    "print(\"NaN values in payment_type_imputed:\", df['payment_type_imputed'].isna().sum())\n",
    "\n",
    "# Compare distributions\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': raw_df['payment_type'].value_counts(normalize=True, dropna=False),\n",
    "    'Probabilistic': df['payment_type_imputed'].value_counts(normalize=True),\n",
    "})\n",
    "print(\"\\nComparison of original and probabilistic imputed distributions:\")\n",
    "display(comparison)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "comparison['Original'].plot(kind='bar', ax=ax1, title='Original Distribution')\n",
    "comparison['Probabilistic'].plot(kind='bar', ax=ax2, title='Probabilistic Imputed Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning for `congestion_surcharge`\n",
    "- `congestion_surcharge` has 4324 null values\n",
    "- we notice **strong** positive correlation between `congestion_surcharge` and `DOLocationID`\n",
    "- since `congestion_surcharge` is categorical with 3 possible values (0, 0.75, 2.5, 2.75), we can use the median value of `congestion_surcharge` for each `DOLocationID` to impute the missing values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning for `congestion_surcharge`\n",
    "print(\"Congestion Surcharge Frequency:\")\n",
    "display(raw_df['congestion_surcharge'].value_counts(dropna=False))\n",
    "\n",
    "print(\"Congestion Surcharge Distribution:\")\n",
    "display(raw_df['congestion_surcharge'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "raw_df[['congestion_surcharge', 'PULocationID', 'DOLocationID']].corr()\n",
    "\n",
    "correlation = raw_df[['congestion_surcharge', 'PULocationID', 'DOLocationID']].corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and show median congestion_surcharge for each DOLocationID\n",
    "median_by_location = df.groupby('DOLocationID')['congestion_surcharge'].median()\n",
    "display(median_by_location)\n",
    "\n",
    "# incase there is DOLocationID that have all rows with null `congestion_surcharge`, we can use overall median as fallback\n",
    "overall_median = df['congestion_surcharge'].median()\n",
    "\n",
    "# Define a function to get the median congestion_surcharge for a given DOLocationID\n",
    "# if location_id is not in the dataset, use 0 as default value\n",
    "def get_median_surcharge(location_id):\n",
    "    if location_id in median_by_location and not pd.isnull(median_by_location[location_id]):\n",
    "        return median_by_location[location_id]\n",
    "    else:\n",
    "        return overall_median\n",
    "\n",
    "\n",
    "# Impute each missing values based on respective DOLocationID\n",
    "df['congestion_surcharge_imputed'] = df.apply(\n",
    "    lambda row: get_median_surcharge(row['DOLocationID']) if pd.isnull(row['congestion_surcharge']) else row['congestion_surcharge'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compare original and imputed distributions\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': raw_df['congestion_surcharge'].value_counts(normalize=True, dropna=False),\n",
    "    'Imputed': df['congestion_surcharge_imputed'].value_counts(normalize=True),\n",
    "})\n",
    "\n",
    "print(\"Comparison of original and imputed congestion_surcharge distributions:\")\n",
    "display(comparison)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': raw_df['congestion_surcharge'].value_counts(normalize=True, dropna=False),\n",
    "    'Imputed': df['congestion_surcharge_imputed'].value_counts(normalize=True),\n",
    "})\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# plot original data\n",
    "comparison['Original'].plot(kind='bar', ax=ax1, color='skyblue')\n",
    "ax1.set_title('Original Congestion Surcharge Distribution')\n",
    "ax1.set_xlabel('Congestion Surcharge')\n",
    "ax1.set_ylabel('Proportion')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for imputed data\n",
    "comparison['Imputed'].plot(kind='bar', ax=ax2, color='lightgreen')\n",
    "ax2.set_title('Imputed Congestion Surcharge Distribution')\n",
    "ax2.set_xlabel('Congestion Surcharge')\n",
    "ax2.set_ylabel('Proportion')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning for `passenger_count`\n",
    "- column type is continuous\n",
    "- better to use regression to impute the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_columns = df.columns.drop(['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'ehail_fee', ])\n",
    "\n",
    "print(\"Passenger Count Distribution:\")\n",
    "display(df['passenger_count'].value_counts(normalize=True, dropna=False))\n",
    "\n",
    "print(\"\\nPassenger Count Summary Statistics:\")\n",
    "display(df['passenger_count'].describe())\n",
    "# ================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check normality of `passenger_count`\n",
    "\n",
    "- Histogram\n",
    "- Shapiro-Wilk test\n",
    "- Kolmogorov-Smirnov test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN values for the analysis\n",
    "passenger_count = df['passenger_count'].dropna()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(passenger_count, bins=30, edgecolor='black')\n",
    "plt.title('Histogram of Passenger Count')\n",
    "plt.xlabel('Passenger Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "statistic, p_value = stats.shapiro(passenger_count)\n",
    "print(f\"Shapiro-Wilk test statistic: {statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "if p_value > 0.05:\n",
    "    print(\"The passenger_count data appears to be normally distributed (fail to reject H0)\")\n",
    "else:\n",
    "    print(\"The passenger_count data does not appear to be normally distributed (reject H0)\")\n",
    "\n",
    "from scipy.stats import kstest\n",
    "\n",
    "ks_statistic, p_value = kstest(df['passenger_count'].dropna(), 'norm')\n",
    "print(f\"\\nKolmogorov-Smirnov test statistic: {ks_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "if p_value > 0.05:\n",
    "    print(\"The data appears to be normally distributed (fail to reject H0)\")\n",
    "else:\n",
    "    print(\"The data does not appear to be normally distributed (reject H0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a copy of the dataframe for imputation\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# Median Imputation\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "df['passenger_count_imputed'] = median_imputer.fit_transform(df[['passenger_count']])\n",
    "\n",
    "# Compare distributions after imputation\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.histplot(df['passenger_count'].dropna(), kde=True)\n",
    "plt.title('Original (non-null values)')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.histplot(df['passenger_count_imputed'], kde=True)\n",
    "plt.title('Median Imputation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Original (non-null values):\")\n",
    "print(df['passenger_count'].dropna().describe())\n",
    "\n",
    "print(\"\\nMedian Imputation:\")\n",
    "print(df['passenger_count_imputed'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Drop `ehail_fee` column as it does not have any values\n",
    "## - Convert `lpep_pickup_datetime` and `lpep_dropoff_datetime` to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ehail_fee'], inplace=True)\n",
    "df['lpep_pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "df['lpep_dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "\n",
    "# get month and year from `lpep_pickup_datetime`\n",
    "df['month'] = df['lpep_pickup_datetime'].dt.month\n",
    "df['year'] = df['lpep_pickup_datetime'].dt.year\n",
    "\n",
    "display(df.describe())\n",
    "display(df.isnull().sum())\n",
    "\n",
    "display(df['month'].value_counts())\n",
    "display(df['year'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean outliers for trip_distance\n",
    "1. From the summary statistics, we can see that the mean trip_distance is 8.11 miles, and the max is 100 miles\n",
    "2. But std is 585.11 miles, which is way too high and almost impossible\n",
    "3. Max trip_distance is 120,098.84 miles which is clearly impossible, because it's larger than the circumference of Earth (~24,901 miles) and NYC's maximum possible trip distance would be around 30-35 miles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['trip_distance'] <= 100]\n",
    "\n",
    "print(\"\\nCleaned trip distance statistics:\")\n",
    "print(df['trip_distance'].describe())\n",
    "\n",
    "sns.histplot(data=df, x='trip_distance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch NYC Taxi Zone Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = pd.read_csv('taxi_zone_lookup.csv')\n",
    "df_zone.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new clean dataframe\n",
    "\n",
    "- Copy last processed `df`\n",
    "- Add `duration` column (in minutes)\n",
    "- Replace null columns with imputed ones\n",
    "- add `avg_speed` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "df_clean.drop(columns=['store_and_fwd_flag','RatecodeID','passenger_count', 'payment_type','trip_type', 'congestion_surcharge'], inplace=True)\n",
    "df_clean.rename(columns={'store_and_fwd_flag_imputed':'store_and_fwd_flag', 'RatecodeID_imputed':'RatecodeID', 'passenger_count_imputed':'passenger_count', 'payment_type_imputed':'payment_type', 'trip_type_imputed':'trip_type', 'congestion_surcharge_imputed':'congestion_surcharge'}, inplace=True)\n",
    "\n",
    "display(df_clean.dtypes)\n",
    "\n",
    "# derive duration column\n",
    "df_clean['duration'] = df_clean['lpep_dropoff_datetime'] - df_clean['lpep_pickup_datetime']\n",
    "df_clean['duration'] = df_clean['duration'].dt.total_seconds() / 60\n",
    "\n",
    "# add time-related columns\n",
    "df_clean['hour'] = df_clean['lpep_pickup_datetime'].dt.hour\n",
    "df_clean['day_of_week'] = df_clean['lpep_pickup_datetime'].dt.day_name()\n",
    "\n",
    "# No more null values\n",
    "print(\"\\nNull values summary:\")\n",
    "display(df_clean.isnull().sum())\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average speed (mph) = distance / (duration in hours)\n",
    "# Convert duration from minutes to hours by dividing by 60\n",
    "df_clean['avg_speed'] = df_clean['trip_distance'] / (df_clean['duration'] / 60)\n",
    "\n",
    "display(df_clean[df_clean['duration'] != 0]['avg_speed'].describe())\n",
    "\n",
    "# incase duration is 0, replace infinite values with mean, i.e. 13.5 mph\n",
    "df_clean['avg_speed'] = df_clean['avg_speed'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "display(df_clean[df_clean['avg_speed'] > 100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Revenue Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Revenue Analysis\n",
    "fig, axes = plt.subplots(2,2, figsize=(20, 17))\n",
    "\n",
    "# Fare Amount Distribution\n",
    "sns.histplot(data=df_clean, x='fare_amount', bins=100, ax=axes[0,0])\n",
    "axes[0,0].set_title('Distribution of Fare Amounts')\n",
    "axes[0,0].set_xlabel('Fare Amount ($)')\n",
    "axes[0,0].set_xlim(0, 100)\n",
    "\n",
    "# Total Amount by Payment Type\n",
    "sns.boxplot(data=df_clean, x='payment_type', y='total_amount', ax=axes[0,1])\n",
    "axes[0,1].set_title('Total Amount by Payment Type')\n",
    "axes[0,1].set_xlabel('Payment Type')\n",
    "axes[0,1].set_ylabel('Total Amount ($)')\n",
    "\n",
    "# Average Fare by Location\n",
    "location_avg_fare = df_clean.groupby('PULocationID')['fare_amount'].mean().sort_values(ascending=False).head(10)\n",
    "location_avg_fare.plot(kind='bar', ax=axes[1,0])\n",
    "axes[1,0].set_title('Top 10 Pickup Locations by Average Fare')\n",
    "axes[1,0].set_xlabel('Location ID')\n",
    "axes[1,0].set_ylabel('Average Fare ($)')\n",
    "\n",
    "# Tips Distribution\n",
    "sns.histplot(data=df_clean, x='tip_amount', bins=50, ax=axes[1,1])\n",
    "axes[1,1].set_title('Distribution of Tips')\n",
    "axes[1,1].set_xlabel('Tip Amount ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Trip Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Trip Efficiency Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Trip Duration vs Fare\n",
    "sns.scatterplot(data=df_clean, x='duration', y='fare_amount', alpha=0.5, ax=axes[0,0])\n",
    "axes[0,0].set_title('Trip Duration vs Fare Amount')\n",
    "axes[0,0].set_xlabel('Duration (minutes)')\n",
    "axes[0,0].set_ylabel('Fare Amount ($)')\n",
    "\n",
    "# what plot shall i add to complete 4 subplots?\n",
    "# Average Speed Distribution\n",
    "sns.histplot(data=df_clean[(df_clean['duration'] > 0) & (df_clean['avg_speed'] < 100)], x='avg_speed', bins=20, ax=axes[0,1])\n",
    "axes[0,1].set_xlim(0, 100)\n",
    "axes[0,1].set_title('Distribution of Average Speeds')\n",
    "axes[0,1].set_xlabel('Average Speed (mph)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "# trip duration vs distance\n",
    "sns.scatterplot(data=df_clean, x='duration', y='trip_distance', alpha=0.5, ax=axes[1,0])\n",
    "axes[1,0].set_title('Trip Duration vs Distance')\n",
    "axes[1,0].set_xlabel('Duration (minutes)')\n",
    "axes[1,0].set_ylabel('Distance (miles)')\n",
    "\n",
    "# Passenger Count Distribution\n",
    "sns.countplot(data=df_clean, x='passenger_count', ax=axes[1,1])\n",
    "axes[1,1].set_title('Distribution of Passenger Count')\n",
    "axes[1,1].set_xlabel('Number of Passengers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.histplot(df_clean['duration'], bins=30, kde=True)\n",
    "plt.title('Trip Duration Distribution')\n",
    "plt.xlabel('Duration (minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Trip Distance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_clean['trip_distance'].describe())\n",
    "df_clean[df_clean['trip_distance'] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Trip Distance Distribution\n",
    "sns.histplot(data=df_clean, x='trip_distance', bins=50, ax=axes[0,0])\n",
    "axes[0,0].set_title('Distribution of Trip Distances')\n",
    "axes[0,0].set_xlabel('Distance (miles)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_xlim(0, 20)  # Focus on typical trip distances\n",
    "\n",
    "# 2. Distance vs Fare Amount\n",
    "sns.scatterplot(data=df_clean, x='trip_distance', y='fare_amount', alpha=0.5, ax=axes[0,1])\n",
    "axes[0,1].set_title('Trip Distance vs Fare Amount')\n",
    "axes[0,1].set_xlabel('Distance (miles)')\n",
    "axes[0,1].set_ylabel('Fare Amount ($)')\n",
    "\n",
    "# 3. Average Distance by Hour\n",
    "hourly_distance = df_clean.groupby('hour')['trip_distance'].mean()\n",
    "hourly_distance.plot(kind='line', marker='o', ax=axes[1,0])\n",
    "axes[1,0].set_title('Average Trip Distance by Hour')\n",
    "axes[1,0].set_xlabel('Hour of Day')\n",
    "axes[1,0].set_ylabel('Average Distance (miles)')\n",
    "\n",
    "# 4. Average Distance by Day\n",
    "daily_distance = df_clean.groupby('day_of_week')['trip_distance'].mean()\n",
    "daily_distance.plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Average Trip Distance by Day')\n",
    "axes[1,1].set_xlabel('Day of Week')\n",
    "axes[1,1].set_ylabel('Average Distance (miles)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistance Analysis Metrics:\")\n",
    "print(f\"Average trip distance: {df_clean['trip_distance'].mean():.2f} miles\")\n",
    "print(f\"Median trip distance: {df_clean['trip_distance'].median():.2f} miles\")\n",
    "print(f\"Most common trip distance range: {df_clean['trip_distance'].mode().iloc[0]:.2f} miles\")\n",
    "print(f\"Maximum trip distance: {df_clean['trip_distance'].max():.2f} miles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Time Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 7))\n",
    "\n",
    "# Hourly Trip Distribution\n",
    "hourly_trips = df_clean.groupby('hour')['fare_amount'].mean()\n",
    "hourly_trips.plot(kind='line', marker='o', ax=axes[0,0 ])\n",
    "axes[0,0].set_title('Average Fare by Hour of Day')\n",
    "axes[0,0].set_xlabel('Hour')\n",
    "axes[0,0].set_ylabel('Average Fare ($)')\n",
    "\n",
    "# Daily Trip Distribution\n",
    "daily_trips = df_clean.groupby('day_of_week')['fare_amount'].mean()\n",
    "daily_trips.plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('Average Fare by Day of Week')\n",
    "axes[0,1].set_xlabel('Day of Week')\n",
    "axes[0,1].set_ylabel('Average Fare ($)')\n",
    "\n",
    "# Sum of fare_amount by day and hour\n",
    "hourly_trips = df_clean.groupby('hour')['fare_amount'].sum()\n",
    "hourly_trips.plot(kind='line', marker='o', ax=axes[1,0])\n",
    "axes[1,0].set_title('Average Fare by Hour of Day')\n",
    "axes[1,0].set_xlabel('Hour')\n",
    "axes[1,0].set_ylabel('Average Fare ($)')\n",
    "\n",
    "daily_trips = df_clean.groupby('day_of_week')['fare_amount'].sum()\n",
    "daily_trips.plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Average Fare by Day of Week')\n",
    "axes[1,1].set_xlabel('Day of Week')\n",
    "axes[1,1].set_ylabel('Average Fare ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(2,1,figsize=(10, 8))\n",
    "\n",
    "# Hourly Passenger Count\n",
    "hourly_passengers = df_clean.groupby('hour')['passenger_count'].mean()\n",
    "hourly_passengers.plot(kind='line',\n",
    "                      marker='o',\n",
    "                      ax=axes[0])\n",
    "axes[0].set_title('Average Passengers by Hour')\n",
    "\n",
    "# Revenue by Day and Hour, using total_amount\n",
    "pivot_data = df_clean.pivot_table(index='hour',\n",
    "                                 columns='day_of_week',\n",
    "                                 values='total_amount',\n",
    "                                 aggfunc='mean')\n",
    "sns.heatmap(pivot_data, \n",
    "            cmap='YlOrRd',\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('Average Revenue by Day and Hour')\n",
    "\n",
    "# add legend for heatmap\n",
    "cbar = axes[1].collections[0].colorbar\n",
    "cbar.set_label('Average Revenue ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Driver satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots\n",
    "fig, axes = plt.subplots(3,2, figsize=(14, 10))\n",
    "sns.scatterplot(data=df_clean, \n",
    "                x='fare_amount',\n",
    "                y='tip_amount',\n",
    "                alpha=0.5,\n",
    "                ax=axes[0,0])\n",
    "axes[0,0].set_title('Fare Amount vs Tip Amount')\n",
    "\n",
    "# analyze tips by hour\n",
    "df_clean.groupby('hour')['tip_amount'].mean().plot(kind='line', marker='o', ax=axes[0,1])\n",
    "axes[0,1].set_title('Average Tips by Hour')\n",
    "axes[0,1].set_xlabel('Hour')\n",
    "axes[0,1].set_ylabel('Average Tips ($)')\n",
    "\n",
    "\n",
    "# analyze tips by day of week\n",
    "df_clean.groupby('day_of_week')['tip_amount'].mean().plot(kind='bar', ax=axes[  1,0])\n",
    "axes[1,0].set_title('Average Tips by Day of Week')\n",
    "axes[1,0].set_xlabel('Day of Week')\n",
    "axes[1,0].set_ylabel('Average Tips ($)')\n",
    "\n",
    "\n",
    "# analyze tips by passenger count\n",
    "df_clean.groupby('passenger_count')['tip_amount'].mean().plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Average Tips by Passenger Count')\n",
    "axes[1,1].set_xlabel('Passenger Count')\n",
    "axes[1,1].set_ylabel('Average Tips ($)')\n",
    "\n",
    "# analyze tips by PULocationID\n",
    "df_clean.groupby('PULocationID')['tip_amount'].mean().sort_values(ascending=False).head(10).plot(kind='bar', ax=axes[2,0])\n",
    "axes[2,0].set_title('Top 10 Pickup Locations by Average Tips')\n",
    "axes[2,0].set_xlabel('Location ID')\n",
    "axes[2,0].set_ylabel('Average Tips ($)')\n",
    "\n",
    "# analyze tips by DOLocationID\n",
    "df_clean.groupby('DOLocationID')['tip_amount'].mean().sort_values(ascending=False).head(10).plot(kind='bar', ax=axes[2,1])\n",
    "axes[2,1].set_title('Top 10 Dropoff Locations by Average Tips')\n",
    "axes[2,1].set_xlabel('Location ID')\n",
    "axes[2,1].set_ylabel('Average Tips ($)')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import plugins\n",
    "import branca.colormap as cm\n",
    "\n",
    "# Read the shapefile\n",
    "nyc_zones = gpd.read_file('taxi_zones_spatial/taxi_zones.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_stats = df_clean.groupby('PULocationID')['fare_amount'].mean().reset_index()\n",
    "nyc_zones = nyc_zones.merge(zone_stats, left_on='OBJECTID', right_on='PULocationID', how='left')\n",
    "\n",
    "# Start with New York location\n",
    "m = folium.Map(location=[40.7128, -74.0060], zoom_start=10)\n",
    "\n",
    "# Create a colormap\n",
    "colormap = cm.LinearColormap(\n",
    "    colors=['yellow', 'red'],\n",
    "    vmin=zone_stats['fare_amount'].min(),\n",
    "    vmax=zone_stats['fare_amount'].max()\n",
    ")\n",
    "m.add_child(colormap)\n",
    "\n",
    "# Add GeoJson data to folium map\n",
    "folium.GeoJson(\n",
    "    nyc_zones,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': colormap(feature['properties']['fare_amount']) if feature['properties']['fare_amount'] else 'grey',\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.7\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['zone', 'fare_amount', 'borough'],\n",
    "        aliases=['Zone:', 'Avg Fare:', 'Borough:'],\n",
    "        localize=True\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Save the map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 routes with highest number of trips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from folium import plugins\n",
    "\n",
    "\n",
    "# Convert the coordinate system to lat/long (EPSG:4326)\n",
    "nyc_zones_2 = nyc_zones.to_crs(epsg=4326)\n",
    "\n",
    "# Calculate top 10 routes based on frequency\n",
    "top_routes = df_clean.groupby(['PULocationID', 'DOLocationID']).size().reset_index(name='count')\n",
    "top_routes = top_routes.sort_values('count', ascending=False).head(10)\n",
    "\n",
    "# Merge with zone names\n",
    "top_routes = top_routes.merge(\n",
    "    df_clean[['PULocationID', 'PUZone']].drop_duplicates(),\n",
    "    on='PULocationID'\n",
    ").merge(\n",
    "    df_clean[['DOLocationID', 'DOZone']].drop_duplicates(),\n",
    "    on='DOLocationID'\n",
    ")\n",
    "\n",
    "# Now create the map\n",
    "m = folium.Map(location=[40.785091, -73.878284], zoom_start=12)\n",
    "# generate 10 different colors\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange', 'yellow', 'pink', 'brown', 'gray', 'cyan']\n",
    "\n",
    "\n",
    "unique_pickup_zones = top_routes['PULocationID'].unique()\n",
    "unique_dropoff_zones = top_routes['DOLocationID'].unique()\n",
    "all_involved_zones = np.union1d(unique_pickup_zones, unique_dropoff_zones)\n",
    "\n",
    "# Add zone polygons of each used zones\n",
    "for zone_id in all_involved_zones:\n",
    "    try:\n",
    "        zone_geom = nyc_zones_2[nyc_zones_2['OBJECTID'] == zone_id]\n",
    "        if not zone_geom.empty:\n",
    "            # Add zone polygon with light fill and border\n",
    "            folium.GeoJson(\n",
    "                zone_geom,\n",
    "                style_function=lambda x: {\n",
    "                    'fillColor': '#0000FF', \n",
    "                    'color': '#000000', \n",
    "                    'weight': 2,\n",
    "                    'fillOpacity': 0.3\n",
    "                },\n",
    "                tooltip=folium.GeoJsonTooltip(\n",
    "                    fields=['zone'],\n",
    "                    aliases=['Zone:'],\n",
    "                    localize=True\n",
    "                )\n",
    "            ).add_to(m)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing zone {zone_id}: {e}\")\n",
    "\n",
    "# add the routes\n",
    "for idx, route in top_routes.iterrows():\n",
    "    try:\n",
    "        # Get coordinates\n",
    "        pu_geom = nyc_zones_2[nyc_zones_2['OBJECTID'] == route['PULocationID']]['geometry'].iloc[0]\n",
    "        do_geom = nyc_zones_2[nyc_zones_2['OBJECTID'] == route['DOLocationID']]['geometry'].iloc[0]\n",
    "        \n",
    "        # get x,y coordinates\n",
    "        pu_lat, pu_lon = pu_geom.centroid.y, pu_geom.centroid.x\n",
    "        do_lat, do_lon = do_geom.centroid.y, do_geom.centroid.x\n",
    "        current_color = colors[idx % len(colors)]\n",
    "\n",
    "        # Add pickup marker\n",
    "        folium.CircleMarker(\n",
    "            location=[pu_lat, pu_lon],\n",
    "            radius=5,\n",
    "            color=current_color,\n",
    "            fill=True,\n",
    "            fillOpacity=1.0,\n",
    "            popup=f\"Pickup: {route['PUZone']}\\nTrips: {route['count']}\",\n",
    "            weight=3\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # add dropoff marker\n",
    "        folium.CircleMarker(\n",
    "            location=[do_lat, do_lon],\n",
    "            radius=5,\n",
    "            color=current_color,\n",
    "            fill=True,\n",
    "            fillOpacity=1.0,\n",
    "            popup=f\"Dropoff: {route['DOZone']}\\nTrips: {route['count']}\",\n",
    "            weight=3\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Add line\n",
    "        line = folium.PolyLine(\n",
    "            locations=[[pu_lat, pu_lon], [do_lat, do_lon]],\n",
    "            color=current_color,\n",
    "            weight=4,\n",
    "            opacity=1.0,\n",
    "            popup=f\"Route {idx+1}: {route['count']} trips\"\n",
    "        ).add_to(m)\n",
    "\n",
    "        # add arrow icon on the line\n",
    "        plugins.PolyLineTextPath(\n",
    "            line,\n",
    "            text='âžœ',\n",
    "            repeat=True,\n",
    "            offset=8,\n",
    "            attributes={'font-size': '24px', 'fill': current_color}\n",
    "        ).add_to(m)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing route {idx+1}: {e}\")\n",
    "\n",
    "# add shape for the zones used\n",
    "\n",
    "# legend\n",
    "legend_html = f'''\n",
    "<div style=\"position: fixed; \n",
    "            top: 20px; right: 20px; width: 200px;\n",
    "            border: 1px solid rgba(0,0,0,0.2);\n",
    "            border-radius: 4px;\n",
    "            z-index: 9999; \n",
    "            background-color: rgba(255,255,255,0.9);\n",
    "            padding: 8px;  \n",
    "            font-size: 11px;\">\n",
    "     <p style=\"margin: 0 0 5px 0;\"><b>Top {len(top_routes)} Routes</b></p>\n",
    "     <div style=\"max-height: 200px; overflow-y: auto;\">\n",
    "'''\n",
    "\n",
    "#\n",
    "for idx, route in top_routes.iterrows():\n",
    "    current_color = colors[idx % len(colors)]\n",
    "\n",
    "    legend_html += f'''\n",
    "    <div style=\"margin-bottom: 5px;\">\n",
    "        <i class=\"fa fa-circle fa-1x\" style=\"color:{current_color}\"></i> Route {idx+1}: {route['count']}<br>\n",
    "        <span style=\"margin-left: 12px\">From: {route['PUZone']}</span><br>\n",
    "        <span style=\"margin-left: 12px\">To: {route['DOZone']}</span>\n",
    "    </div>\n",
    "    '''\n",
    "\n",
    "legend_html += '''\n",
    "     </div>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Display the map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "1. The highest number of trips are from East Harlem North to East Harlem South. It's \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('NYC_TLC_cleaned_zone.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
